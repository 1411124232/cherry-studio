id: minimax/minimax-m1:extended
canonical_slug: minimax/minimax-m1
hugging_face_id: ''
name: 'MiniMax: MiniMax M1 (extended)'
type: chat
created: 1750200414
description: |-
  MiniMax-M1 is a large-scale, open-weight reasoning model designed for extended context and high-efficiency inference. It leverages a hybrid Mixture-of-Experts (MoE) architecture paired with a custom "lightning attention" mechanism, allowing it to process long sequences—up to 1 million tokens—while maintaining competitive FLOP efficiency. With 456 billion total parameters and 45.9B active per token, this variant is optimized for complex, multi-step reasoning tasks.

  Trained via a custom reinforcement learning pipeline (CISPO), M1 excels in long-context understanding, software engineering, agentic tool use, and mathematical reasoning. Benchmarks show strong performance across FullStackBench, SWE-bench, MATH, GPQA, and TAU-Bench, often outperforming other open models like DeepSeek R1 and Qwen3-235B.
context_length: 128000
architecture:
  modality: text->text
  input_modalities:
    - text
  output_modalities:
    - text
  tokenizer: Other
  instruct_type: null
pricing:
  prompt: '0.00000055'
  completion: '0.0000022'
  input_cache_read: ''
  input_cache_write: ''
  request: '0'
  image: '0'
  web_search: '0'
  internal_reasoning: '0'
  unit: 1
  currency: USD
supported_parameters:
  - tools
  - tool_choice
  - max_tokens
  - temperature
  - top_p
  - reasoning
  - include_reasoning
  - structured_outputs
  - stop
  - frequency_penalty
  - presence_penalty
  - seed
  - top_k
  - min_p
  - repetition_penalty
  - logit_bias
model_provider: minimax
