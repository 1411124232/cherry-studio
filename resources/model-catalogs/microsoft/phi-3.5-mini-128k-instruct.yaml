id: microsoft/phi-3.5-mini-128k-instruct
canonical_slug: microsoft/phi-3.5-mini-128k-instruct
hugging_face_id: microsoft/Phi-3.5-mini-instruct
name: 'Microsoft: Phi-3.5 Mini 128K Instruct'
type: chat
created: 1724198400
description: |-
  Phi-3.5 models are lightweight, state-of-the-art open models. These models were trained with Phi-3 datasets that include both synthetic data and the filtered, publicly available websites data, with a focus on high quality and reasoning-dense properties. Phi-3.5 Mini uses 3.8B parameters, and is a dense decoder-only transformer model using the same tokenizer as [Phi-3 Mini](/models/microsoft/phi-3-mini-128k-instruct).

  The models underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures. When assessed against benchmarks that test common sense, language understanding, math, code, long context and logical reasoning, Phi-3.5 models showcased robust and state-of-the-art performance among models with less than 13 billion parameters.
context_length: 128000
architecture:
  modality: text->text
  input_modalities:
    - text
  output_modalities:
    - text
  tokenizer: Other
  instruct_type: phi3
pricing:
  prompt: '0.0000001'
  completion: '0.0000001'
  input_cache_read: ''
  input_cache_write: ''
  request: '0'
  image: '0'
  web_search: '0'
  internal_reasoning: '0'
  unit: 1
  currency: USD
supported_parameters:
  - tools
  - tool_choice
  - max_tokens
  - temperature
  - top_p
model_provider: microsoft
