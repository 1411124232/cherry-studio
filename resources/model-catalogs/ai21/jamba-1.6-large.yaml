id: ai21/jamba-1.6-large
canonical_slug: ai21/jamba-1.6-large
hugging_face_id: ai21labs/AI21-Jamba-Large-1.6
name: 'AI21: Jamba 1.6 Large'
type: chat
created: 1741905173
description: |-
  AI21 Jamba Large 1.6 is a high-performance hybrid foundation model combining State Space Models (Mamba) with Transformer attention mechanisms. Developed by AI21, it excels in extremely long-context handling (256K tokens), demonstrates superior inference efficiency (up to 2.5x faster than comparable models), and supports structured JSON output and tool-use capabilities. It has 94 billion active parameters (398 billion total), optimized quantization support (ExpertsInt8), and multilingual proficiency in languages such as English, Spanish, French, Portuguese, Italian, Dutch, German, Arabic, and Hebrew.

  Usage of this model is subject to the [Jamba Open Model License](https://www.ai21.com/licenses/jamba-open-model-license).
context_length: 256000
architecture:
  modality: text->text
  input_modalities:
    - text
  output_modalities:
    - text
  tokenizer: Other
  instruct_type: null
pricing:
  prompt: '0.000002'
  completion: '0.000008'
  input_cache_read: ''
  input_cache_write: ''
  request: '0'
  image: '0'
  web_search: '0'
  internal_reasoning: '0'
  unit: 1
  currency: USD
supported_parameters:
  - tools
  - tool_choice
  - max_tokens
  - temperature
  - top_p
  - stop
model_provider: ai21
